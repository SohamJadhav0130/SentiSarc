{"cells":[{"cell_type":"markdown","source":["## Prediction Model"],"metadata":{"id":"G2W0YMuM_UAx"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"M8oUc77Lcz3j","executionInfo":{"status":"ok","timestamp":1687955109174,"user_tz":-330,"elapsed":488,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3v5oGKb1cz3q","executionInfo":{"status":"ok","timestamp":1687955113820,"user_tz":-330,"elapsed":4666,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import gensim\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n","from tensorflow.keras.layers import Embedding\n","from keras.initializers import Constant\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"s1JtLuagcz3s","executionInfo":{"status":"ok","timestamp":1687955113821,"user_tz":-330,"elapsed":15,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["def clean_text(text):\n","    text = text.lower()\n","\n","    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","    text = pattern.sub('', text)\n","    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n","    emoji = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","\n","    text = emoji.sub(r'', text)\n","    text = text.lower()\n","    text = re.sub(r\"i'm\", \"i am\", text)\n","    text = re.sub(r\"he's\", \"he is\", text)\n","    text = re.sub(r\"she's\", \"she is\", text)\n","    text = re.sub(r\"that's\", \"that is\", text)\n","    text = re.sub(r\"what's\", \"what is\", text)\n","    text = re.sub(r\"where's\", \"where is\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"won't\", \"will not\", text)\n","    text = re.sub(r\"don't\", \"do not\", text)\n","    text = re.sub(r\"did't\", \"did not\", text)\n","    text = re.sub(r\"can't\", \"can not\", text)\n","    text = re.sub(r\"it's\", \"it is\", text)\n","    text = re.sub(r\"couldn't\", \"could not\", text)\n","    text = re.sub(r\"have't\", \"have not\", text)\n","    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n","    return text"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"y_M7S8iWdL4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687955189199,"user_tz":-330,"elapsed":75390,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}},"outputId":"a884f3dc-6cdd-4754-a39f-0c32972fb79f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aTmBvd06cz3u","executionInfo":{"status":"ok","timestamp":1687955191230,"user_tz":-330,"elapsed":2045,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["data_1 = pd.read_json(\"/content/drive/MyDrive/Sarcasm detection/Sarcasm_Headlines_Dataset.json\", lines=True)\n","data_2 = pd.read_json(\"/content/drive/MyDrive/Sarcasm detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n","\n","data =  pd.concat([data_1, data_2])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zsMHXzTaBH-2","executionInfo":{"status":"ok","timestamp":1687955191947,"user_tz":-330,"elapsed":723,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["import nltk\n","nltk.data.path.append('/content/drive/MyDrive/nltk_data')"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('words')\n","nltk.download('stopwords')"],"metadata":{"id":"s1Cqg4JE8ImR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687955192476,"user_tz":-330,"elapsed":538,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}},"outputId":"9d7d2eec-d39c-4d6a-cd98-668d7f313496"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zDb2luxacz3v","executionInfo":{"status":"ok","timestamp":1687955212850,"user_tz":-330,"elapsed":20381,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","def CleanTokenize(df):\n","    head_lines = list()\n","    lines = df[\"headline\"].values.tolist()\n","\n","    for line in lines:\n","        line = clean_text(line)\n","        # tokenize the text\n","        tokens = word_tokenize(line)\n","        # remove puntuations\n","        table = str.maketrans('', '', string.punctuation)\n","        stripped = [w.translate(table) for w in tokens]\n","        # remove non alphabetic characters\n","        words = [word for word in stripped if word.isalpha()]\n","        stop_words = set(stopwords.words(\"english\"))\n","        # remove stop words\n","        words = [w for w in words if not w in stop_words]\n","        head_lines.append(words)\n","    return head_lines\n","\n","head_lines = CleanTokenize(data)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wdLdJ6pvcz3w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687955214167,"user_tz":-330,"elapsed":1330,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}},"outputId":"e8b3c35b-f232-44a5-def7-ac71ca8bcd2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["unique tokens -  28657\n","vocab size - 28658\n"]}],"source":["validation_split = 0.2\n","max_length = 25\n","\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(head_lines)\n","sequences = tokenizer_obj.texts_to_sequences(head_lines)\n","\n","word_index = tokenizer_obj.word_index\n","print(\"unique tokens - \",len(word_index))\n","vocab_size = len(tokenizer_obj.word_index) + 1\n","print('vocab size -', vocab_size)\n","\n","lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n","sentiment =  data['is_sarcastic'].values\n","\n","indices = np.arange(lines_pad.shape[0])\n","np.random.shuffle(indices)\n","lines_pad = lines_pad[indices]\n","sentiment = sentiment[indices]\n","\n","num_validation_samples = int(validation_split * lines_pad.shape[0])\n","\n","X_train_pad = lines_pad[:-num_validation_samples]\n","y_train = sentiment[:-num_validation_samples]\n","X_test_pad = lines_pad[-num_validation_samples:]\n","y_test = sentiment[-num_validation_samples:]"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"j6zFTF34cz3y","executionInfo":{"status":"ok","timestamp":1687955216315,"user_tz":-330,"elapsed":2153,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["\n","import tensorflow as tf\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Sarcasm detection/model.h5')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"y9BpvM2Qcz3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687955217122,"user_tz":-330,"elapsed":813,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}},"outputId":"b168a8f5-cae4-4da3-fb5c-60abd2b9e629"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 423ms/step\n","It's a sarcasm!\n"]}],"source":["import pandas as pd\n","def predict_sarcasm(s):\n","    x_final = pd.DataFrame({\"headline\":[s]})\n","    test_lines = CleanTokenize(x_final)\n","    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n","    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","    pred = loaded_model.predict(test_review_pad)\n","    pred*=100\n","    if pred[0][0]>=50: return \"It's a sarcasm!\"\n","    else: return \"It's not a sarcasm.\"\n","\n","# Make predictions using the loaded model\n","prediction = predict_sarcasm(\"I just won million dollars\")\n","print(prediction)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"CBYksdUuj_5Y","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1687955217124,"user_tz":-330,"elapsed":25,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}},"outputId":"f6a77aeb-8601-417c-fa66-5e063854a8c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 26ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["\"It's not a sarcasm.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["predict_sarcasm(\"I want million dollars.\")"]},{"cell_type":"markdown","source":["##Getting Data for Analysis"],"metadata":{"id":"eDBBx5UK_fbX"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"PP9S_SFFB-Qu","executionInfo":{"status":"ok","timestamp":1687955714126,"user_tz":-330,"elapsed":497019,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","import gspread\n","from google.auth import default\n","creds, _ = default()\n","\n","gc = gspread.authorize(creds)\n","\n","wb = gc.open_by_key(\"1A5b0qRBglWZJVczNz5dhO4Rdreent-nQCeQ4C_rqtP0\")\n","\n","ws = wb.worksheet('Sheet1')\n","\n","# get_all_values gives a list of rows.\n","rows = ws.get_all_values()"]},{"cell_type":"code","source":["rows = pd.DataFrame(rows,columns=['username','FullName','likes','comments','Date','description','type','hashtag','location','polarity'])"],"metadata":{"id":"DAQyB4P-tPXT","executionInfo":{"status":"ok","timestamp":1687955714127,"user_tz":-330,"elapsed":51,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"YLg70RflHK5U","executionInfo":{"status":"ok","timestamp":1687955714128,"user_tz":-330,"elapsed":16,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SX8eihvYUn5B","executionInfo":{"status":"ok","timestamp":1687955714129,"user_tz":-330,"elapsed":15,"user":{"displayName":"Soham Jadhav","userId":"06103833801616233309"}}},"outputs":[],"source":["def predict_sarcasm(df):\n","    x_final = pd.DataFrame({\"headline\": df['description']})\n","    test_lines = CleanTokenize(x_final)\n","    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n","    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","    preds = loaded_model.predict(test_review_pad)\n","    sarcasm_scores = preds[:, 0] * 100\n","    sarcasm_labels = ['Negative' if score >= 63 else 'It\\'s not a sarcasm.' for score in sarcasm_scores]\n","    sarcasm_codes = [0 if label == 'Negative' else 1 for label in sarcasm_labels]\n","    output_df = pd.DataFrame({'Sentence': df['description'], 'Sentiment': sarcasm_labels, 'Score': sarcasm_codes})\n","    output_df = pd.concat([df, output_df['Sentiment'], output_df['Score']], axis=1)\n","    return output_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkT9jUBmU31N"},"outputs":[],"source":["output_df = predict_sarcasm(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwNQIz-8vKnf"},"outputs":[],"source":["df = output_df['description']\n","df = pd.DataFrame({'description':df})"]},{"cell_type":"code","source":["df.drop"],"metadata":{"id":"rRBvWO7323Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ug1rMnvTU_1o"},"outputs":[],"source":["sarcasm_df = output_df[output_df['Score'] == 0]\n","print(sarcasm_df['description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uYdvU3TDfYL"},"outputs":[],"source":["non_sarcasm_df = output_df[output_df['Score']==1]\n","df = non_sarcasm_df['description']\n","df = pd.DataFrame({'description':df})"]},{"cell_type":"markdown","metadata":{"id":"3aoh6AjuRU6W"},"source":["## Defining the library of Positive, Negative and Connecting Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlQSYhIVRSoT"},"outputs":[],"source":["#defining positive, negative and connectors lists of words\n","\n","positive = [\"love\",\"like\",\"happy\",\"good\",\"excellent\",\"great\",\"nice\",\"wonderful\",\"fantastic\",\"amazing\",\"awesome\",\"terrific\",\"fabulous\",\"delightful\",\"pleasure\",\"enjoy\",\n","    \"satisfy\",\"smile\",\"laugh\",\"success\",\"kind\",\"generous\",\"gracious\",\"forgiving\",\"compassionate\",\"understanding\",\"trust\",\"loyal\",\"honest\",\"fair\",\"brave\",\n","    \"adventurous\",\"confident\",\"creative\",\"smart\",\"intelligent\",\"insightful\",\"inspiring\",\"motivated\",\"passionate\",\"productive\",\"focused\",\"disciplined\",\"healthy\",\n","    \"energetic\",\"athletic\",\"beautiful\",\"attractive\",\"fashionable\",\"stylish\",\"successful\",\"wealthy\",\"rich\",\"prosperous\",\"abundant\",\"blessed\",\"fortunate\",\"lucky\",\n","    \"grateful\",\"content\",\"peaceful\",\"calm\",\"relaxed\",\"tranquil\",\"serene\",\"happy\",\"joyful\",\"blissful\",\"ecstatic\",\"hopeful\",\"optimistic\",\"courageous\",\"confident\",\n","    \"proud\",\"admire\",\"respect\",\"appreciate\",\"significant\",\"glad\",\"perfect\",\"rapturous\", \"fascinating\", \"enriching\", \"tranquilizing\", \"blissful\", \"serendipitous\", \"inspirational\", \"pleasurable\", \"upbeat\", \"ecstatic\",\n","    \"delicious\", \"exhilarating\", \"glamorous\", \"majestic\", \"stunning\", \"irresistible\", \"breathtaking\", \"prodigious\", \"miraculous\", \"miracle\",\"thrilling\",\n","    \"mind-blowing\", \"exquisite\", \"alluring\", \"extraordinary\", \"spectacular\", \"outstanding\", \"remarkable\", \"unforgettable\", \"marvelous\", \"unbeatable\", \"brilliant\", \"genius\", \"stellar\", \"fantabulous\", \"incredible\", \"sublime\",\n","    \"superb\", \"terrific\", \"top-notch\", \"wondrous\",\"\"]\n","\n","negative = [\"not\",\"neither\",\"don't\",\"didn't\",\"wouldn't\",\"shouldn't\",\"haven't\",\"hadn't\",\"can't\",\"hate\",\"unhappy\",\"bad\",\"terrible\",\"uncomfortable\",\"horrible\",\"awful\",\"disgusting\",\"ugly\",\"painful\",\n","    \"sorrow\",\"hurt\",\"upset\",\"frustrated\",\"angry\",\"annoyed\",\"disappointed\",\"regret\",\"guilt\",\"shame\",\"fear\",\"scared\",\"anxious\",\"worried\",\"nervous\",\"doubt\",\"insecure\",\n","    \"jealous\",\"envy\",\"betray\",\"abandon\",\"alone\",\"lonely\",\"depressed\",\"sad\",\"miserable\",\"gloomy\",\"hopeless\",\"despair\",\"powerless\",\"worthless\",\"rejected\",\"humiliated\",\n","    \"insulted\",\"offended\",\"disrespected\",\"unappreciated\",\"frustrated\",\"overwhelmed\",\"exhausted\",\"tired\",\"bored\",\"stressed\",\"irritated\",\"annoying\",\"negative\",\"critical\",\n","    \"mean\",\"cruel\",\"selfish\",\"arrogant\",\"ignorant\",\"stupid\",\"lazy\",\"unmotivated\",\"unproductive\",\"unsuccessful\",\"poor\",\"struggle\",\"failure\",\"loss\",\"difficult\",\"tragic\",\n","    \"disastrous\",\"devastating\",\"pathetic\",\"disappointing\",\"regretful\",\"heartbroken\",\"wounded\",\"insensitive\",\"impolite\",\"unfriendly\",\"cynical\",\"pessimistic\",\"cowardly\",\n","    \"indecisive\",\"irresponsible\",\"foolish\",\"weak\",\"worthless\",\"incompetent\",\"dependent\",\"unreliable\",\"untrustworthy\",\"unhealthy\",\"unfit\",\"unattractive\",\"poor\",\"broke\",\n","    \"struggling\",\"difficult\",\"disgusted\",\"offended\",\"trapped\",\"stuck\",\"lonely\",\"empty\",\"lost\",\"confused\",\"ashamed\",\"humiliated\",\"horrified\",\"disillusioned\",\"depressed\",\n","    \"unhappy\",\"bitter\",\"enraged\",\"resentful\",\"discontented\",\"displeased\",\"worried\",\"nervous\",\"fearful\",\"anxious\",\"terrified\",\"petrified\",\"timid\",\"shy\",\"insecure\",\n","    \"uncertain\",\"doubtful\",\"skeptical\",\"disbelieving\",\"inattentive\",\"distracted\",\"absent-minded\",\"forgetful\",\"neglectful\",\"careless\",\"reckless\",\"irresponsible\",\n","    \"impulsive\",\"careless\",\"inconsiderate\",\"thoughtless\",\"selfish\",\"self-centered\",\"egotistical\",\"arrogant\",\"ignorant\",\"inflexible\",\"stubborn\",\"rigid\",\"boring\",\"weird\"\n","    \"monotonous\",\"repetitive\",\"tedious\",\"dull\",\"uninteresting\",\"unexciting\",\"mundane\",\"trivial\",\"unimportant\",\"insignificant\",\"meaningless\",\"worthless\",\"disappointed\"]\n","\n","#connectors are the subordinating conjunctions which join two or more phrases + punctuations\n","connectors = [\"for\",\"and\",\"nor\",\"but\",\"or\",\"yet\",\"so\",\",\",\";\"]"]},{"cell_type":"markdown","metadata":{"id":"xSzwCoTlSjfS"},"source":["## Function to encode the sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JffppTryStyd"},"outputs":[],"source":["def encodeSentence(sentence):\n","  # words = sentence.split()\n","  words = re.findall(r\"[\\w']+|[^\\w\\s]\", sentence)\n","  encoded_sentence = []\n","  for word in words:\n","    if any(char.isdigit() or char.isalpha() for char in word):\n","      if word in positive:\n","        encoded_sentence.append(2)\n","\n","      elif word in negative:\n","        encoded_sentence.append(0)\n","\n","      elif word in connectors:\n","        encoded_sentence.append(3)\n","\n","      else:\n","        encoded_sentence.append(1)\n","\n","    else:\n","      encoded_sentence.append(3)\n","\n","  return encoded_sentence"]},{"cell_type":"markdown","metadata":{"id":"zxguMK5MTQC0"},"source":["## Function to convert the encoded sentence into a tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGVJWjGGTbMq"},"outputs":[],"source":["def createTree(encoded_sentence):\n","  tree_diagram =[]\n","  N = len(encoded_sentence)\n","  count = 0\n","\n","  for i in range(0,N+1):\n","    tree_diagram.append([])\n","\n","  for code in encoded_sentence:\n","    if count == N:\n","      tree_diagram[count-1].append(code)\n","    elif count == 0:\n","      tree_diagram[count].append(code)\n","      tree_diagram[count].append(None)\n","    else:\n","      tree_diagram[count].append(code)\n","      tree_diagram[count].append(None)\n","\n","    count = count+1\n","\n","  return tree_diagram"]},{"cell_type":"markdown","metadata":{"id":"TWalSTetTg0T"},"source":["## Function to compute the values for sentiment tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwbwHjPfTpo_"},"outputs":[],"source":["def calculateValue(temp,tree):\n","  result = 1\n","\n","  if tree[temp][0]==0 and tree[temp][1]==1:\n","        result=0\n","  elif tree[temp][0] == 0 and tree[temp][1] == 0:\n","        result = 2\n","  elif tree[temp][0] == 1 and tree[temp][1] == 1:\n","        result = 1\n","  elif tree[temp][0] == 1 and tree[temp][1] == 0:\n","        result = 0\n","  elif tree[temp][0] == 1 and tree[temp][1] == 2:\n","        result = 2\n","  elif tree[temp][0] == 2 and tree[temp][1] == 2:\n","        result = 2\n","  elif tree[temp][0] == 2 and tree[temp][1] == 1:\n","      result = 2\n","  elif tree[temp][0] == 0 and tree[temp][1] == 2:\n","        result = 0\n","  elif tree[temp][0] == 2 and tree[temp][1] == 0:\n","        result = 0\n","\n","  return result"]},{"cell_type":"markdown","metadata":{"id":"IcfgaAuRTufb"},"source":["## Function to calculate the Sentiment from sentiment tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wy9-Y5ROT1qi"},"outputs":[],"source":["def calculateSentiment(temp):\n","  result = 1\n","\n","  if sentiment_tree[temp][0]==0 and sentiment_tree[temp][1]==1:\n","        result=0\n","  elif sentiment_tree[temp][0] == 0 and sentiment_tree[temp][1] == 0:\n","        result = 0\n","  elif sentiment_tree[temp][0] == 1 and sentiment_tree[temp][1] == 1:\n","        result = 1\n","  elif sentiment_tree[temp][0] == 1 and sentiment_tree[temp][1] == 0:\n","        result = 0\n","  elif sentiment_tree[temp][0] == 1 and sentiment_tree[temp][1] == 2:\n","        result = 2\n","  elif sentiment_tree[temp][0] == 2 and sentiment_tree[temp][1] == 2:\n","        result = 2\n","  elif sentiment_tree[temp][0] == 2 and sentiment_tree[temp][1] == 1:\n","      result = 2\n","  elif sentiment_tree[temp][0] == 0 and sentiment_tree[temp][1] == 2:\n","        result = 2\n","  elif sentiment_tree[temp][0] == 2 and sentiment_tree[temp][1] == 0:\n","        result = 0\n","\n","  return result"]},{"cell_type":"markdown","metadata":{"id":"8_L2kQy8T8WB"},"source":["## Getting sentences for analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN1xgIXkT7uy"},"outputs":[],"source":["#for predefined list of sentences\n","testSentences = [\n","    \"I absolutely love this toothpaste, it leaves my teeth feeling clean and fresh.\",\n","    \"This toothpaste is terrible, it made my mouth feel dry and uncomfortable.\",\n","    \"I've been using this toothpaste for a week and I haven't noticed any difference in my teeth.\",\n","    \"I'm not a fan of the taste of this toothpaste, but it does the job.\",\n","    \"I've been using this toothpaste for years and I wouldn't use anything else.\",\n","    \"This toothpaste is too expensive for what it does.\",\n","    \"I tried this toothpaste and it gave me a headache.\",\n","    \"This toothpaste has a refreshing minty flavor that I love.\",\n","    \"I bought this toothpaste based on the positive reviews, but I was disappointed.\",\n","    \"I can't stand the taste of this toothpaste, it's way too strong.\",\n","    \"I noticed a significant improvement in my teeth after using this toothpaste for a week.\",\n","    \"This toothpaste leaves a weird aftertaste in my mouth.\",\n","    \"I'm not sure if this toothpaste is making a difference, but it does taste good.\",\n","    \"This toothpaste is great for sensitive teeth, it doesn't cause any pain.\",\n","    \"I wouldn't recommend this toothpaste to anyone, it just doesn't work.\",\n","    \"I've been using this toothpaste for a month and I've noticed a big difference in the whiteness of my teeth.\",\n","    \"This toothpaste is perfect for traveling, it's small and easy to pack.\",\n","    \"I don't like the texture of this toothpaste, it's too gritty.\",\n","    \"I was hesitant to try this toothpaste, but I'm glad I did. It works really well.\",\n","    \"This toothpaste is very affordable and does a great job of cleaning my teeth.\"\n","]\n","\n","# # or use this for getting sentences from google sheet\n","\n","# from google.colab import auth\n","# auth.authenticate_user()\n","\n","# import gspread\n","# from google.auth import default\n","# creds, _ = default()\n","\n","# gc = gspread.authorize(creds)\n","\n","# wb = gc.open_by_key('1Mlc--MW-RMXPlgC6YTbi561gcGyuehB0QcIwUJhiYW8')\n","\n","# ws = wb.worksheet('Sheet1')\n","\n","# # Extract all the tweets\n","# testSentences = ws.col_values(1)"]},{"cell_type":"markdown","metadata":{"id":"Cn9YLB3hUfwb"},"source":["## Prediction\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["##Sarcasm Model"],"metadata":{"id":"M-lwZ7xbN0Jh"}},{"cell_type":"code","source":["#Live Prediction\n","def prediction(s):\n","    x_final = pd.DataFrame({\"headline\":[s]})\n","    test_lines = CleanTokenize(x_final)\n","    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n","    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","    pred = loaded_model.predict(test_review_pad)\n","    pred*=100\n","    if pred[0][0]>=50: return \"It's a sarcasm!\"\n","    else: return \"It's not a sarcasm.\""],"metadata":{"id":"XOX8-xY8J-CH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction(\"I was depressed. He asked me to be happy. I am not depressed anymore\")"],"metadata":{"id":"13IPIlXoNxAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction(\"I love Colgate but I dont like its taste\")"],"metadata":{"id":"9kKoIYA4NxX1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sentiment Tree"],"metadata":{"id":"xmBLep01N4EB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"akt3ekatbYCh"},"outputs":[],"source":["import re\n","\n","result_list = ['Negative', 'Neutral', 'Positive']\n","final = []\n","\n","testSentences = [\"I love Colgate but I don't like its taste\"]\n","\n","for sentence in testSentences:\n","  # sentence = row['description']\n","  tree = createTree(encodeSentence(sentence))\n","  sentiment = []\n","  result = []\n","  encoded_sentence = encodeSentence(sentence)\n","  # print(sentence, encoded_sentence)\n","  list_splits = []\n","  start_index = 0\n","  #splitting the sentences by connectors\n","  for i, value in enumerate(encoded_sentence):\n","      if value == 3:\n","          list_splits.append(encoded_sentence[start_index:i])\n","          start_index = i + 1\n","  list_splits.append(encoded_sentence[start_index:])\n","  # print(encoded_sentence, list_splits)\n","  # Create a tree for each split\n","  for split in list_splits:\n","    if split:\n","      tree = createTree(split)\n","      # print(split, tree)\n","      #getting sentiment of splits\n","      N = len(split)\n","      temp = N\n","      # print(N)\n","      for i in range(0, N):\n","        temp = temp -1\n","        if i == 0:\n","          tree[temp][1]=1\n","        else:\n","          tree[temp][1] = calculateValue(temp+1,tree)\n","    sentiment.append(calculateValue(temp,tree))\n","    # print(tree, sentiment)\n","\n","#       # print(\"Encoded Sentence:\", encoded_sentence)\n","#       # print(\"Split Lists:\", list_splits)\n","#       # print(\"Sentiment:\", sentiment)\n","\n","  #creating tree for sentiments\n","  sentiment_tree = createTree(sentiment)\n","  # print(sentiment, sentiment_tree)\n","  # print(len(sentiment_tree))\n","  N = len(sentiment)\n","  # print(N)\n","  temp = N\n","\n","  for i in range(0,N):\n","    # print(sentiment_tree)\n","    temp=temp-1\n","    # print(temp, i)\n","    if i == 0:\n","      sentiment_tree[temp][1]=1\n","    else:\n","      sentiment_tree[temp][1] = calculateSentiment(temp+1)\n","  # print(sentiment, sentiment_tree)\n","\n","  final.append(calculateSentiment(temp))\n","\n","print(final)"]},{"cell_type":"markdown","metadata":{"id":"d_sPhCyiUwF8"},"source":["## Printing the dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8iOAULtU1VE"},"outputs":[],"source":["for i in final:\n","  if i == 0:\n","    print(f'{testSentences[0]} - Negative')\n","  if i == 1:\n","    print(f'{testSentences[0]} - Neutral')\n","  if i == 2:\n","    print(f'{testSentences[0]} - Positive')"]},{"cell_type":"markdown","source":["#Analysis"],"metadata":{"id":"8Cax-F9E1NUy"}},{"cell_type":"code","source":["neutral_len = len(final_sentiment[final_sentiment['Sentiment'] == 'Neutral'])\n","positive_len = len(final_sentiment[final_sentiment['Sentiment'] == 'Positive'])\n","negative_len = len(final_sentiment[final_sentiment['Sentiment'] == 'Negative'])\n","\n","print('Length of Neutral column:', neutral_len)\n","print('Length of Positive column:', positive_len)\n","print('Length of Negative column:', negative_len)"],"metadata":{"id":"MqDjsfQC5TH5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sentiment Pie Chart"],"metadata":{"id":"hSm7zL_sv3XZ"}},{"cell_type":"code","source":["final_sentiment['Score'] = pd.to_numeric(final_sentiment['Score'], errors='coerce')\n","# Convert polarity values less than 0 to -1 and greater than 0 to 1\n","# df['Polarity'] = df['Polarity'].apply(lambda x: -1 if x < 0 else 1 if x > 0 else 0)\n","\n","# Group the data by polarity\n","df_polarity = final_sentiment.groupby('Score').size().reset_index(name='counts')\n","\n","# Set the labels and values for the pie chart\n","labels = ['Negative', 'Neutral', 'Positive']\n","colors = ['#FF0000','#FFFF00','#228B22']\n","values = df_polarity['counts'].tolist()\n","\n","# Plot the pie chart\n","plt.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n","plt.axis('equal')\n","plt.show()"],"metadata":{"id":"QvR3HANk0GYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["average_sentiment = final_sentiment['Score'].mean()\n","\n","average_sentiment"],"metadata":{"id":"QYyHyg4m5GmX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sentiment Line Graph for Each hastag"],"metadata":{"id":"Z6IQp3U-w2Gn"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import datetime\n","\n","# convert \"Date\" column to datsetime object\n","final_sentiment[\"Date\"] = pd.to_datetime(final_sentiment[\"Date\"])\n","\n","# extract month from \"Date\" column\n","final_sentiment[\"Month\"] = final_sentiment[\"Date\"].dt.month\n","\n","# group by \"hashtags\" and \"Month\" and calculate mean sentiment\n","grouped = final_sentiment.groupby([\"hashtag\", \"Month\"])[\"Score\"].mean().reset_index()\n","\n","# loop through each unique hashtag and plot line graph\n","for hashtag in grouped[\"hashtag\"].unique():\n","    df = grouped[grouped[\"hashtag\"] == hashtag]\n","    plt.figure()  # create a new figure for each hashtag\n","    plt.plot(df[\"Month\"], df[\"Score\"])\n","    plt.xlabel(\"Month\")\n","    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n","    plt.ylabel(\"Sentiment\")\n","    plt.title(f\"Sentiment per Month for {hashtag}\")\n","    plt.show()  # show the plot for each hashtag\n"],"metadata":{"id":"v4J8Z1FjtCUO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sentiment Distribution for each Hastag\n"],"metadata":{"id":"O0MiYIfWxOPs"}},{"cell_type":"code","source":["df = final_sentiment"],"metadata":{"id":"2p6yrqV_zMUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a dictionary of color codes\n","colors = {'Positive': 'green', 'Neutral': \"#e6d800\", 'Negative': 'red'}\n","\n","# create a DataFrame with the count of each sentiment for each hashtag\n","df_sentiment_count = df.groupby(['hashtag', 'Sentiment'])['Sentiment'].count().unstack('Sentiment').fillna(0)\n","\n","# create stacked bar chart\n","df_sentiment_count.plot(kind='bar', stacked=True, figsize=(10, 6), color=colors)\n","plt.title('Sentiment Distribution for each Hashtag')\n","plt.xlabel('Hashtag')\n","plt.ylabel('Count')\n","plt.show()\n"],"metadata":{"id":"trAfmOYCzd8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1doPvKqo9KWe"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["G2W0YMuM_UAx","eDBBx5UK_fbX","3aoh6AjuRU6W","xSzwCoTlSjfS","zxguMK5MTQC0","TWalSTetTg0T","IcfgaAuRTufb","8_L2kQy8T8WB","Cn9YLB3hUfwb","d_sPhCyiUwF8","8Cax-F9E1NUy","hSm7zL_sv3XZ","Z6IQp3U-w2Gn","O0MiYIfWxOPs"],"provenance":[{"file_id":"1Gq64kIaUW8eRwoqyXczT5w4hiBLQwOVs","timestamp":1681280462203},{"file_id":"1Y8062nXAmZEhpolCbUMQUbpt_6BQnxjB","timestamp":1679981347844}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}